## What is Preference Ranking?

With the growing use of Artificial Intelligence, Large Language Models have become increasingly important in powering chatbots, virtual assistants and other intelligent applications. While these models give impressive responses, it is crucial to consistently fine tune them through training and evaluation to maintain the quality, factuality, accuracy and relevance of outputs. This is where **Preference Ranking** becomes crucial.

Preference Ranking is a widely used method  to evaluate and compare multiple responses generated by LLMs. Responses are evaluated against a rubric and then ranked on a Likert Scale. It serves as a reward system where the model learns that responses ranked higher are good responses and it tunes itself to generate highly ranked responses in future interactions. The ranking is also followed by a justification which is backed by evidence borrowing from all the individual rubrics the response is evaluated against. This justification serves as ordered feedback that trains the model to make tailored adjustments. The model learns from detailed explanations and makes targeted changes performing better in complex output patterns.

<img height="329" width="602" src="${PRIVATE_IMAGE_INTRO_1}" />

Weâ€™ll go through each of these steps in detail in the following modules and learn how to fine tune LLMs. Here's a quick quiz for you to catch up on things discussed till now:

<a href="https://app.soulhq.ai/" style="padding: 10px; background-color: #364BC9; color: white; text-decoration: none; border-radius: 5px;"> Take the Quiz </a>